{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction\n",
    "This notebook goes through multi-processing of NLP for aiming to analyse the dependency of David Bowie with the Media's\n",
    "\n",
    "Specifically, we'll be walking through:\n",
    "\n",
    " - **Getting the data** - in this case, we'll be get the data from a dataset in .csv\n",
    " - **Cleaning the data** - we will walk through popular text pre-processing techniques\n",
    " - **Organizing the data** - we will organize the cleaned data into a way that is easy to input into other algorithms\n",
    "\n",
    "The output of this notebook will be clean, organized data in two standard text formats:\n",
    "\n",
    "  - Corpus - a collection of text\n",
    "  - Document-Term Matrix  word counts in matrix format\n",
    "\n",
    "\n",
    "In each formats, we'll divide in 3 category:\n",
    "\n",
    "  - The press - the magazine from the text\n",
    "  - The year of publish - The year of the interview\n",
    "  - The album - Each interview is linked with an album\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "We would like to know if trough the decade, the review about Bowie evolute.\n",
    "\n",
    "# Get the data\n",
    "\n",
    "Luckily, we already have a raw csv, with all our data ready. I have to import the csv, and transform them in dataframe\n",
    "for each categories.\n",
    "\n",
    "As we decided before we have to divide the dataset in 3 high-level category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-825da6057c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'data/bowie_txt_analysis.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "file = r'data/bowie_txt_analysis.csv'\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# Pickle all the data\n",
    "all_text = open('./data/pickle/all_text.pkl', 'wb')\n",
    "pickle.dump(df, all_text)\n",
    "\n",
    "# Create separate df for each category\n",
    "df_press = df[['press', 'texte', 'date']]\n",
    "df_album = df[['album', 'texte', 'date']]\n",
    "\n",
    "def format_date(date):\n",
    "    new_date = int(date[-2:])\n",
    "    if new_date > 20:\n",
    "        new_date = f'19{new_date}'\n",
    "    elif new_date == 2 or new_date == 3:\n",
    "        new_date = f'200{new_date}'\n",
    "    else:\n",
    "        new_date = f'20{new_date}'\n",
    "    return int(new_date)\n",
    "\n",
    "format_date_round = lambda x: format_date(x)\n",
    "df_press.date = df_press.date.apply(format_date_round)\n",
    "df_press = df_press.groupby(['date','texte', 'press']).size().reset_index()\n",
    "\n",
    "df_press_70 = df_press[df_press['date'].between(1970, 1979)]\n",
    "df_press_70 = df_press_70.groupby(['press'])['texte'].apply(' '.join).reset_index()\n",
    "df_press_80 = df_press[df_press['date'].between(1980, 1989)]\n",
    "df_press_80 = df_press_80.groupby(['press'])['texte'].apply(' '.join).reset_index()\n",
    "df_press_90 = df_press[df_press['date'].between(1990, 1999)]\n",
    "df_press_90 = df_press_90.groupby(['press'])['texte'].apply(' '.join).reset_index()\n",
    "df_press_00 = df_press[df_press['date'].between(2000, 2009)]\n",
    "df_press_00 = df_press_00.groupby(['press'])['texte'].apply(' '.join).reset_index()\n",
    "df_press_10 = df_press[df_press['date'].between(2010, 2019)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data\n",
    "\n",
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n",
    "\n",
    "**Common data cleaning steps on all text**:\n",
    "\n",
    "- Make text all lower case\n",
    "- Remove punctuation\n",
    "- Remove numerical values\n",
    "- Remove common non-sensical text (/n)\n",
    "- Tokenize text\n",
    "- Remove stop words\n",
    "\n",
    "\n",
    "**More data cleaning steps after tokenization**:\n",
    "- Stemming / lemmatization\n",
    "- Parts of speech tagging\n",
    "- Create bi-grams or tri-grams\n",
    "- Deal with typos\n",
    "- And more...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    \"\"\"Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = text.lstrip(' ')\n",
    "    return text\n",
    "\n",
    "def clean_text_round2(text):\n",
    "    \"\"\"Get rid of some additional punctuation and non-sensical text that was missed the first time around.\"\"\"\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_redundant_word(text):\n",
    "    stopwords = ['bowie', 'like', 'bowies', 'just', 'album', 'david', 'time', 'new']\n",
    "    querywords = text.split()\n",
    "    resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "    result = ' '.join(resultwords)\n",
    "    return result\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "round2 = lambda x: clean_text_round2(x)\n",
    "round_stop = lambda x: remove_redundant_word(x)\n",
    "\n",
    "df_press_70['texte'] = pd.DataFrame(df_press_70.texte.apply(round1))\n",
    "df_press_70['texte'] = pd.DataFrame(df_press_70.texte.apply(round2))\n",
    "df_press_70['texte'] = pd.DataFrame(df_press_70.texte.apply(round_stop))\n",
    "\n",
    "df_press_80['texte'] = pd.DataFrame(df_press_80.texte.apply(round1))\n",
    "df_press_80['texte'] = pd.DataFrame(df_press_80.texte.apply(round2))\n",
    "df_press_80['texte'] = pd.DataFrame(df_press_80.texte.apply(round_stop))\n",
    "\n",
    "df_press_90['texte'] = pd.DataFrame(df_press_90.texte.apply(round1))\n",
    "df_press_90['texte'] = pd.DataFrame(df_press_90.texte.apply(round2))\n",
    "df_press_90['texte'] = pd.DataFrame(df_press_90.texte.apply(round_stop))\n",
    "\n",
    "df_press_00['texte'] = pd.DataFrame(df_press_00.texte.apply(round1))\n",
    "df_press_00['texte'] = pd.DataFrame(df_press_00.texte.apply(round2))\n",
    "df_press_00['texte'] = pd.DataFrame(df_press_00.texte.apply(round_stop))\n",
    "\n",
    "# Pickle press df\n",
    "press_pkl = open('./data/pickle/press.pkl', 'wb')\n",
    "pickle.dump(df_press, press_pkl)\n",
    "\n",
    "press_70_pkl = open('./data/pickle/decade/press_70.pkl', 'wb')\n",
    "pickle.dump(df_press_70, press_70_pkl)\n",
    "\n",
    "press_80_pkl = open('./data/pickle/decade/press_80.pkl', 'wb')\n",
    "pickle.dump(df_press_80, press_80_pkl)\n",
    "\n",
    "press_90_pkl = open('./data/pickle/decade/press_90.pkl', 'wb')\n",
    "pickle.dump(df_press_90, press_90_pkl)\n",
    "\n",
    "press_00_pkl = open('./data/pickle/decade/press_00.pkl', 'wb')\n",
    "pickle.dump(df_press_00, press_00_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing The Data\n",
    "I mentioned earlier that the output of this notebook will be clean, organized data in two standard text formats:\n",
    "\n",
    " - **Corpus** - a collection of text\n",
    " - **Document**-Term Matrix - word counts in matrix format\n",
    "\n",
    "## Document-Term Matrix\n",
    "For many of the techniques we'll be using in future notebooks, the text must be tokenized, meaning broken down into smaller pieces. The most common tokenization technique is to break down text into words. We can do this using scikit-learn's CountVectorizer, where every row will represent a different document and every column will represent a different word.\n",
    "\n",
    "In addition, with CountVectorizer, we can remove stop words. Stop words are common words that add no additional meaning to text such as 'a', 'the', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "## for the 70's\n",
    "data_press_70_cv = cv.fit_transform(df_press_70['texte'])\n",
    "data_press_70_dtm = pd.DataFrame(data_press_70_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_press_70_dtm.index = df_press_70.press\n",
    "data_press_70_dtm_pkl = open('./data/pickle/decade/press_70_dtm.pkl', 'wb')\n",
    "pickle.dump(data_press_70_dtm, data_press_70_dtm_pkl)\n",
    "\n",
    "## for the 80's\n",
    "data_press_80_cv = cv.fit_transform(df_press_80['texte'])\n",
    "data_press_80_dtm = pd.DataFrame(data_press_80_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_press_80_dtm.index = df_press_80.press\n",
    "data_press_80_dtm_pkl = open('./data/pickle/decade/press_80_dtm.pkl', 'wb')\n",
    "pickle.dump(data_press_80_dtm, data_press_80_dtm_pkl)\n",
    "\n",
    "## for the 90's\n",
    "data_press_90_cv = cv.fit_transform(df_press_90['texte'])\n",
    "data_press_90_dtm = pd.DataFrame(data_press_90_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_press_90_dtm.index = df_press_90.press\n",
    "data_press_90_dtm_pkl = open('./data/pickle/decade/press_90_dtm.pkl', 'wb')\n",
    "pickle.dump(data_press_90_dtm, data_press_90_dtm_pkl)\n",
    "\n",
    "## for the 00's\n",
    "data_press_00_cv = cv.fit_transform(df_press_00['texte'])\n",
    "data_press_00_dtm = pd.DataFrame(data_press_00_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_press_00_dtm.index = df_press_00.press\n",
    "data_press_00_dtm_pkl = open('./data/pickle/decade/press_00_dtm.pkl', 'wb')\n",
    "pickle.dump(data_press_90_dtm, data_press_00_dtm_pkl)\n",
    "\n",
    "# Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\n",
    "import pickle\n",
    "pickle.dump(cv, open(\"./data/pickle/cv.pkl\", \"wb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
